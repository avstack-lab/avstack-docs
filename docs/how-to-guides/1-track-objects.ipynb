{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253c80b9",
   "metadata": {},
   "source": [
    "# Track Objects\n",
    "\n",
    "Let's build on the last perception example and work to integrate longitudinal scenes with a tracker. AVtack provides a few custom tracking implementations with inspiration taken from some literature examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8d0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import avstack\n",
    "import avapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_base = '../../lib-avstack-api/data/'\n",
    "obj_data_dir_k = os.path.join(data_base, 'KITTI/object')\n",
    "raw_data_dir_k = os.path.join(data_base, 'KITTI/raw')\n",
    "obj_data_dir_c = os.path.join(data_base, 'CARLA/ego-lidar')\n",
    "\n",
    "KSM = avapi.kitti.KittiScenesManager(obj_data_dir_k, raw_data_dir_k, convert_raw=False)\n",
    "KDM = KSM.get_scene_dataset_by_index(scene_idx=0)\n",
    "\n",
    "DM = KDM  # let's use kitti for this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e11958",
   "metadata": {},
   "source": [
    "## LiDAR-Based Perception and Tracking\n",
    "\n",
    "Ensure that you have the perception model weights downloaded. Following the installation instructions from the last tutorial should do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05afc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding default threshold of 0.5 with 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/mmdet3d/evaluation/functional/kitti_utils/eval.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def get_thresholds(scores: np.ndarray, num_gt, num_sample_pts=41):\n",
      "/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:92: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/checkpoints/kitti/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    }
   ],
   "source": [
    "result_folder = 'results_lidar'\n",
    "P = avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "        model='pointpillars', dataset='kitti', threshold=0.7, \n",
    "        save_output=True, save_folder=result_folder)\n",
    "T = avstack.modules.tracking.tracker3d.BasicBoxTracker3D(\n",
    "        framerate=DM.framerate, save_output=True, save_folder=result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038e8ce",
   "metadata": {},
   "source": [
    "### Inference over Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496dde50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/108 [00:00<?, ?it/s]/home/spencer/.cache/pypoetry/virtualenvs/avstack-docs-l0eE3ZqO-py3.10/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|█████████████████████████████████████████████████████████████████| 108/108 [00:07<00:00, 14.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# detection and tracking\n",
    "ego_ref = DM.get_ego(frame=0).as_reference()\n",
    "for frame in tqdm(DM.frames):\n",
    "    pc = DM.get_lidar(frame, sensor='main_lidar')\n",
    "    timestamp = DM.framerate * frame\n",
    "    \n",
    "    # -- perception\n",
    "    dets = P(pc, frame=frame, identifier='lidar-detections')\n",
    "    \n",
    "    # -- tracking\n",
    "    tracks = T(detections=dets, t=timestamp, frame=frame, platform=ego_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d119a32",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluation can be accelerated using multiprocessing - this is enabled by default in the following commands. Evaluation over a sequence outputs both per-frame metrics and also aggregates these metrics over the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af94f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 108/108 [00:15<00:00,  7.09it/s]\n",
      "  0%|                                                                           | 0/108 [00:00<?, ?it/s]/home/spencer/.cache/pypoetry/virtualenvs/avstack-docs-l0eE3ZqO-py3.10/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n",
      "100%|█████████████████████████████████████████████████████████████████| 108/108 [00:20<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLEAR Config:\n",
      "THRESHOLD            : 0.5                           \n",
      "PRINT_CONFIG         : True                          \n",
      "\n",
      "IDEucl Config:\n",
      "THRESHOLD            : 0.4                           \n",
      "PRINT_CONFIG         : True                          \n",
      "\n",
      "Evaluating 1 tracker(s) on 1 sequence(s) for 1 class(es) on AvstackTrackDataset dataset using the following metrics: HOTA, CLEAR, VACE, IDEucl, Count\n",
      "\n",
      "\n",
      "Evaluating no-name\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%capture\n",
    "# ^^ suppress output\n",
    "percep_res_frames, percep_res_seq, _ = avapi.evaluation.get_percep_results_from_folder(\n",
    "    DM, P.save_folder, sensor_eval='main_lidar', multiprocess=True)\n",
    "track_res_frames, track_res_seq, track_res_exp = avapi.evaluation.get_track_results_from_folder(\n",
    "    DM, T.save_folder, sensor_eval='main_lidar', multiprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fc900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate perception results:\n",
      " {'tot_TP': 0, 'tot_FP': 0, 'tot_FN': 450, 'tot_T': 450, 'mean_precision': 0.0, 'mean_recall': 0.0} \n",
      "\n",
      "Aggregate tracking results:\n",
      " {'tot_TT': 321, 'tot_FT': 95, 'tot_MT': 129, 'tot_T': 450, 'mean_precision': 0.7638007054673722, 'mean_recall': 0.7189594356261024} \n",
      "\n",
      "Expanded track metrics include:\n",
      " dict_keys(['HOTA_HOTA', 'HOTA_DetA', 'HOTA_AssA', 'HOTA_DetRe', 'HOTA_DetPr', 'HOTA_AssRe', 'HOTA_AssPr', 'HOTA_LocA', 'HOTA_OWTA', 'HOTA_HOTA_TP', 'HOTA_HOTA_FN', 'HOTA_HOTA_FP', 'HOTA_HOTA(0)', 'HOTA_LocA(0)', 'HOTA_HOTALocA(0)', 'CLEAR_MOTA', 'CLEAR_MOTP', 'CLEAR_MODA', 'CLEAR_CLR_Re', 'CLEAR_CLR_Pr', 'CLEAR_MTR', 'CLEAR_PTR', 'CLEAR_MLR', 'CLEAR_sMOTA', 'CLEAR_CLR_F1', 'CLEAR_FP_per_frame', 'CLEAR_MOTAL', 'CLEAR_MOTP_sum', 'CLEAR_CLR_TP', 'CLEAR_CLR_FN', 'CLEAR_CLR_FP', 'CLEAR_IDSW', 'CLEAR_MT', 'CLEAR_PT', 'CLEAR_ML', 'CLEAR_Frag', 'CLEAR_CLR_Frames', 'VACE_STDA', 'VACE_VACE_IDs', 'VACE_VACE_GT_IDs', 'VACE_FDA', 'VACE_num_non_empty_timesteps', 'VACE_ATA', 'VACE_SFDA', 'IDEucl_IDEucl', 'Count_Dets', 'Count_GT_Dets', 'Count_IDs', 'Count_GT_IDs', 'Count_Frames']) \n",
      "\n",
      "     e.g., HOTA_LocA(0): 3.8871622154323147\n"
     ]
    }
   ],
   "source": [
    "print('Aggregate perception results:\\n', percep_res_seq, '\\n')\n",
    "print('Aggregate tracking results:\\n', track_res_seq, '\\n')\n",
    "print('Expanded track metrics include:\\n', track_res_exp.keys(), '\\n')\n",
    "print(f'     e.g., HOTA_LocA(0): {track_res_exp[\"HOTA_LocA(0)\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca89eb8",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize the tracking results by creating a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26df000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# avapi.visualize.replay.replay_track_results(track_res_frames, fig_width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3fd6a",
   "metadata": {},
   "source": [
    "TODO: fix the bottom vs. center discrepancy on the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90783133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# avapi.visualize.replay.replay_track_percep_results(DM, track_res_frames, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ef2b9",
   "metadata": {},
   "source": [
    "## Camera-LiDAR Fusion at the Tracking Level\n",
    "\n",
    "Notice how the precision metric is greatly improved using camera-lidar fusion as opposed to lidar-based tracking alone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b2c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding default threshold of 0.5 with 0.7\n",
      "Loads checkpoint by local backend from path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection/checkpoints/cityscapes/faster_rcnn_r50_fpn_1x_cityscapes_20200502-829424c0.pth\n",
      "Overriding default threshold of 0.5 with 0.7\n",
      "Loads checkpoint by local backend from path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/checkpoints/kitti/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/mmdet3d/utils/setup_env.py:84: UserWarning: The current default scope \"mmdet\" is not \"mmdet3d\", `register_all_modules` will force the currentdefault scope to be \"mmdet3d\". If this is not expected, please set `init_default_scope=False`.\n",
      "  warnings.warn('The current default scope '\n",
      "/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:92: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result_folder = 'results_fusion'\n",
    "\n",
    "P_cam = avstack.modules.perception.object2dfv.MMDetObjectDetector2D(\n",
    "        model='fasterrcnn', dataset='kitti', threshold=0.7, gpu=0,\n",
    "        save_output=True, save_folder=result_folder)\n",
    "\n",
    "\n",
    "P_lid = avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "        model='pointpillars', dataset='kitti', threshold=0.7, gpu=0,\n",
    "        save_output=True, save_folder=result_folder)\n",
    "\n",
    "\n",
    "T_fuse = avstack.modules.tracking.tracker3d.BasicBoxTrackerFusion3Stage(\n",
    "        framerate=DM.framerate, save_output=True, save_folder=result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c50321",
   "metadata": {},
   "source": [
    "### Inference over Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48246aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # detection and tracking\n",
    "# for frame in tqdm(DM.frames):\n",
    "#     timestamp = DM.framerate * frame\n",
    "#     pc = DM.get_lidar(frame, sensor='main_lidar')\n",
    "#     img = DM.get_image(frame, sensor='main_camera')\n",
    "    \n",
    "#     # -- perception\n",
    "#     dets_lid = P_lid(pc, frame=frame, identifier='lidar-detections')\n",
    "#     dets_cam = P_cam(img, frame=frame, identifier='camera-detections')\n",
    "    \n",
    "#     # -- tracking\n",
    "#     tracks = T_fuse(\n",
    "#         detections={'3d':dets_lid, '2d':dets_cam},\n",
    "#         t=timestamp,\n",
    "#         frame=frame,\n",
    "#         platform=pc.reference,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e9ddd",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5a506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# # ^^ suppress output\n",
    "# percep_res_frames, percep_res_seq, _ = avapi.evaluation.get_percep_results_from_folder(\n",
    "#     DM, P_lid.save_folder, sensor_eval='main_lidar')\n",
    "# track_res_frames, track_res_seq, track_res_exp = avapi.evaluation.get_track_results_from_folder(\n",
    "#     DM, T_fuse.save_folder, sensor_eval='main_lidar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e16412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Aggregate perception results:\\n', percep_res_seq, '\\n')\n",
    "# print('Aggregate tracking results:\\n', track_res_seq, '\\n')\n",
    "# print('Expanded track metrics include:\\n', track_res_exp.keys(), '\\n')\n",
    "# print(f'     e.g., HOTA_LocA(0): {track_res_exp[\"HOTA_LocA(0)\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109fa30",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b4f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# avapi.visualize.replay.replay_track_results(track_res_frames, fig_width=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca05873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# avapi.visualize.replay.replay_track_percep_results(DM, track_res_frames, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e346d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
