{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4b5aca",
   "metadata": {},
   "source": [
    "# Designing AVs\n",
    "\n",
    "Now that we've seen how to run perception and tracking in series, we can venture into designing AVs as a stand-alone class. Let's set things up in the usual way. Keep the autoreload if you want to play around with the core or api libraries in editable mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a840ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import avstack\n",
    "import avapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_base = '../../lib-avstack-api/data/'\n",
    "obj_data_dir_k = os.path.join(data_base, 'KITTI/object')\n",
    "raw_data_dir_k = os.path.join(data_base, 'KITTI/raw')\n",
    "obj_data_dir_n = os.path.join(data_base, 'nuScenes')\n",
    "\n",
    "KSM = avapi.kitti.KittiScenesManager(obj_data_dir_k, raw_data_dir_k, convert_raw=False)\n",
    "KDM = KSM.get_scene_dataset_by_index(scene_idx=0)\n",
    "\n",
    "NSM = avapi.nuscenes.nuScenesManager(obj_data_dir_n)\n",
    "NDM = NSM.get_scene_dataset_by_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2e93d",
   "metadata": {},
   "source": [
    "## Subclassing VehicleEgoStack\n",
    "\n",
    "To create an ego vehicle, we subclass off of AVstack's `VehicleEgoStack`. The only methods we need to implement are (1) how to initialize the modules, and (2) how to process the data as it comes in. All the other piping is handled for us by AVstack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061a3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTestAV(avstack.ego.vehicle.VehicleEgoStack):\n",
    "    \"\"\"Our own little AV\"\"\"\n",
    "    \n",
    "    def _initialize_modules(self, *args, framerate=10, lidar_perception=\"pointpillars\",\n",
    "        tracking=\"basic-box-tracker\", dataset=\"kitti\", **kwargs):\n",
    "        \"\"\"Initialize modules\"\"\"\n",
    "        \n",
    "        self.perception = {\n",
    "            \"object_3d\": avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "                model=lidar_perception, dataset=dataset, **kwargs\n",
    "            )\n",
    "        }\n",
    "        self.tracking = avstack.modules.tracking.tracker3d.BasicBoxTracker3D(framerate=framerate, **kwargs)\n",
    "        self.prediction = avstack.modules.prediction.KinematicPrediction(\n",
    "            dt_pred=1.0/5, t_pred_forward=1, **kwargs,\n",
    "        )\n",
    "\n",
    "    def _tick_modules(self, frame, timestamp, data_manager, platform, *args, **kwargs):\n",
    "        dets_3d = self.perception[\"object_3d\"](\n",
    "            data_manager.pop(\"lidar-0\"), frame=frame, identifier=\"lidar_objects_3d\")\n",
    "        tracks_3d = self.tracking(detections=dets_3d, t=timestamp, frame=frame, platform=platform)\n",
    "        # prediction_3d = self.prediction(tracks_3d, frame=frame)  # FYI prediction is currently slow...\n",
    "\n",
    "        # standard packing of outputs\n",
    "        control=None\n",
    "        localization=None\n",
    "        dets_2d=None\n",
    "        tracks_2d=None\n",
    "        prediction_2d=None\n",
    "        prediction_3d=None\n",
    "        plan=None\n",
    "        return control, (localization, dets_2d, dets_3d, tracks_2d, tracks_3d,\\\n",
    "            prediction_2d, prediction_3d, plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934667a9",
   "metadata": {},
   "source": [
    "## Testing our AV\n",
    "\n",
    "To test our AV, we can make a simple data-source agnostic loop that stores data inside a `DataManager` class. Because of the design decision to attach sensor-identifying information onto the sensor data itself, the `DataManager` will be able to automatically allocate the sensor data into the appropriate bin - e.g., so that a sensor with very high rate doesn't impact the data buffer for any other sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30fbca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_our_av(DM, av):\n",
    "    \"\"\"Run an AV through a sequence of data\"\"\"\n",
    "    data_manager = avstack.datastructs.DataManager(max_size=1)  # we either pop or we lose it\n",
    "    for frame in tqdm(DM.frames):\n",
    "        # -- add data --> sensor data has ID attached, so it knows where to go\n",
    "        data_manager.push(DM.get_lidar(frame, sensor=\"main_lidar\"))\n",
    "        data_manager.push(DM.get_image(frame, sensor=\"main_camera\"))\n",
    "        \n",
    "        # -- run next frame\n",
    "        t = DM.framerate * frame\n",
    "        av.tick(frame, t, data_manager, platform=DM.get_calibration(frame, sensor=\"main_lidar\").reference)\n",
    "\n",
    "t_init = 0\n",
    "ego_init = None  # we don't need this for an agent without localization\n",
    "result_folder = 'results_AV'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1991cc",
   "metadata": {},
   "source": [
    "### KITTI\n",
    "\n",
    "Once we define the test loop, all that's left is to initialize our AV and put it through the evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc079791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/checkpoints/kitti/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 108/108 [00:06<00:00, 17.08it/s]\n"
     ]
    }
   ],
   "source": [
    "DM = KDM\n",
    "AV = OurTestAV(t_init, ego_init=None, framerate=DM.framerate, dataset=DM.NAME,\n",
    "                save_output=True, save_folder=result_folder)\n",
    "run_our_av(KDM, AV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1819c90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 108/108 [00:21<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "percep_res_frames, percep_res_seq, _ = avapi.evaluation.get_percep_results_from_folder(\n",
    "    DM, AV.perception['object_3d'].save_folder, sensor_eval='main_lidar', multiprocess=True)\n",
    "# track_res_frames, track_res_seq, track_res_exp = avapi.evaluation.get_track_results_from_folder(\n",
    "#     DM, AV.tracking.save_folder, sensor_eval='main_lidar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0100ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate perception results:\n",
      " {'tot_TP': 0, 'tot_FP': 0, 'tot_FN': 450, 'tot_T': 450, 'mean_precision': 0.0, 'mean_recall': 0.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Aggregate perception results:\\n', percep_res_seq, '\\n')\n",
    "# print('Aggregate tracking results:\\n', track_res_seq, '\\n')\n",
    "# print(f'     e.g., HOTA_LocA(0): {track_res_exp[\"HOTA_LocA(0)\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d0276",
   "metadata": {},
   "source": [
    "### nuScenes\n",
    "\n",
    "Note that the only difference here is that we have to pass in nuScenes as the dataset so the perception models can initialize to the nuScenes weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "608f1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/checkpoints/nuscenes/hv_pointpillars_fpn_sbn-all_fp16_2x8_2x_nus-3d_20201021_120719-269f9dd6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 39/39 [00:02<00:00, 14.32it/s]\n"
     ]
    }
   ],
   "source": [
    "DM = NDM\n",
    "AV = OurTestAV(t_init, ego_init=None, framerate=DM.framerate, dataset=DM.NAME,\n",
    "                save_output=True, save_folder=result_folder)\n",
    "run_our_av(DM, AV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3540575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 39/39 [00:27<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "percep_res_frames, percep_res_seq, _ = avapi.evaluation.get_percep_results_from_folder(\n",
    "    DM, AV.perception['object_3d'].save_folder, sensor_eval='main_lidar', multiprocess=True)\n",
    "# track_res_frames, track_res_seq, track_res_exp = avapi.evaluation.get_track_results_from_folder(\n",
    "#     DM, AV.tracking.save_folder, sensor_eval='main_lidar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2ecbbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate perception results:\n",
      " {'tot_TP': 0, 'tot_FP': 0, 'tot_FN': 2057, 'tot_T': 2057, 'mean_precision': 0.0, 'mean_recall': 0.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Aggregate perception results:\\n', percep_res_seq, '\\n')\n",
    "# print('Aggregate tracking results:\\n', track_res_seq, '\\n')\n",
    "# print(f'     e.g., HOTA_LocA(0): {track_res_exp[\"HOTA_LocA(0)\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c894d",
   "metadata": {},
   "source": [
    "## Exploring Pre-Made AVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fbff1",
   "metadata": {},
   "source": [
    "We've already designed a few AVs we think will be useful while you're spinning up AVstack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fa3a289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VehicleEgoStack',\n",
       " 'PassthroughAutopilotVehicle',\n",
       " 'LidarPerceptionAndTrackingVehicle',\n",
       " 'LidarCollabPerceptionAndTrackingVehicle',\n",
       " 'LidarCameraPerceptionAndTrackingVehicle',\n",
       " 'LidarCamera3DFusionVehicle',\n",
       " 'Level2LidarBasedVehicle',\n",
       " 'Level2GtPerceptionGtLocalization',\n",
       " 'GroundTruthBehaviorAgent',\n",
       " 'GoStraightEgo',\n",
       " 'AutopilotWithCameraPerception']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, inspect\n",
    "\n",
    "clsmembers = [item[0] for item in reversed(inspect.getmembers(avstack.ego.vehicle, inspect.isclass))]\n",
    "clsmembers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544de3b",
   "metadata": {},
   "source": [
    "Later, we'll get to how we can design \"active\" AVs that use perception and tracking information to make plans and control actuators in our environment. But for that, we'll need an AV simulator (or real hardware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1533e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
