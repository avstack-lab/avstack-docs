{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320ea3e9",
   "metadata": {},
   "source": [
    "# Transforming Data\n",
    "\n",
    "One useful element of AVstack is the marriage between data and calibration. You can only imagine the frustration-savings! Let's walk through a few examples where this is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728764c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot import rss library -- don't worry about this unless you need 'safety' evals\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import avstack\n",
    "import avapi\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_base = '../../data'\n",
    "obj_data_dir_k = os.path.join(data_base, 'KITTI/object')\n",
    "raw_data_dir_k = os.path.join(data_base, 'KITTI/raw')\n",
    "obj_data_dir_c = os.path.join(data_base, 'CARLA/object-v1')\n",
    "\n",
    "KSM = avapi.kitti.KittiScenesManager(obj_data_dir_k, raw_data_dir_k, convert_raw=False)\n",
    "KDM = KSM.get_scene_dataset_by_index(scene_idx=0)\n",
    "\n",
    "CSM = avapi.carla.CarlaScenesManager(obj_data_dir_c)\n",
    "CDM = CSM.get_scene_dataset_by_index(0)\n",
    "\n",
    "DM = KDM  # let's use KITTI for this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e13c4",
   "metadata": {},
   "source": [
    "## Representing Objects in Different Sensor Frames\n",
    "\n",
    "All objects in AVstack are specified relative to a reference coordinate frame. That means we can easily acquire object representations in different coordinate frames - the work is performed under-the-hood. The following show the same ground truth object in different coordinate reference frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1713d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box3D=[h: 2.17, w: 1.71, l: 4.33] x (x: -8.35 y: 2.09, z: 24.98)\n",
      "  q: <class 'avstack.geometry.datastructs.Attitude'> - quaternion(0.49074671, -0.48384659, -0.50841785, 0.51630556), ReferenceFrame level 2, x: [0.    0.06  0.905], q: quaternion(0.49951805, 0.49674862, -0.50538759, 0.49830303), v: [0. 0. 0.], acc: [0. 0. 0.], ang: quaternion(1, 0, 0, 0) with reference: ReferenceFrame level 2, x: [0.    0.06  0.905], q: quaternion(0.49951805, 0.49674862, -0.50538759, 0.49830303), v: [0. 0. 0.], acc: [0. 0. 0.], ang: quaternion(1, 0, 0, 0) \n",
      "\n",
      "Box3D=[h: 2.17, w: 1.71, l: 4.33] x (x: 25.21 y: 8.60, z: -1.79)\n",
      "  q: <class 'avstack.geometry.datastructs.Attitude'> - quaternion(0.02130213, 0, 0, -0.99977308), ReferenceFrame level 2, x: [-0.27   0.     0.985], q: quaternion(-0.99991581, 0.00461001, 0.00957989, -0.00743908), v: [0. 0. 0.], acc: [0. 0. 0.], ang: quaternion(1, 0, 0, 0) with reference: ReferenceFrame level 2, x: [-0.27   0.     0.985], q: quaternion(-0.99991581, 0.00461001, 0.00957989, -0.00743908), v: [0. 0. 0.], acc: [0. 0. 0.], ang: quaternion(1, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "frame = 0\n",
    "objects_cam = DM.get_objects(frame=frame, sensor='main_camera')\n",
    "objects_lid = DM.get_objects(frame=frame, sensor='main_lidar')\n",
    "print(objects_cam[0].box3d, '\\n')\n",
    "print(objects_lid[0].box3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d3877",
   "metadata": {},
   "source": [
    "We can verify these are nearly equivalent with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0079d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not exactly the same due to rounding errors!\n",
      "But really close to the same!\n"
     ]
    }
   ],
   "source": [
    "obj_L1_cam = objects_lid[0].deepcopy()\n",
    "obj_L1_cam.change_reference(objects_cam[0].reference, inplace=True)\n",
    "\n",
    "# check exact equality\n",
    "if obj_L1_cam.box3d != objects_cam[0].box3d:\n",
    "    print('Not exactly the same due to rounding errors!')\n",
    "\n",
    "# check approximate equality\n",
    "if obj_L1_cam.box3d.allclose(objects_cam[0].box3d):\n",
    "    print('But really close to the same!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939666d3",
   "metadata": {},
   "source": [
    "## Implicit Conversions\n",
    "\n",
    "Some functions in AVstack are smart enough to know that they should convert under the hood when performing certain operations. One example is checking for bounding box overlap - overlap should be independent of the coordinate frame. A similar example is visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3cfdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion handled under the hood!\n",
      "But again, not exact due to rounding errors\n"
     ]
    }
   ],
   "source": [
    "# did we need to do conversion at all??\n",
    "if objects_lid[0].box3d.allclose(objects_cam[0].box3d):\n",
    "    print('Conversion handled under the hood!')\n",
    "\n",
    "# but again, appoximate\n",
    "if objects_lid[0].box3d != objects_cam[0].box3d:\n",
    "    print('But again, not exact due to rounding errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f125b1",
   "metadata": {},
   "source": [
    "Let's check the bounding box overlap with the IoU (intersection-over-union) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00cae399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking bounding box overlap -- also implicitly converted!\n",
    "objects_lid[0].box3d.IoU(objects_cam[0].box3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcb520",
   "metadata": {},
   "source": [
    "## Applying External Transformations\n",
    "\n",
    "We can apply external transformations (rotations and translations) on any object. Rotations are quaternions and translations are 3-vectors. When applied to objects, in both cases, a new object is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae9758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We rotated! Original object unchanged.\n",
      "Back to the original with inversion\n"
     ]
    }
   ],
   "source": [
    "# -- rotation with a free-standing quaternion\n",
    "q1 = avstack.geometry.transformations.transform_orientation([0, 20*np.pi/180, 0], 'euler', 'quat')\n",
    "rot1 = avstack.geometry.Attitude(q=q1, reference=objects_lid[0].box3d.reference)\n",
    "new_box1 = objects_lid[0].box3d.rotate(rot1, inplace=False)\n",
    "if not new_box1.allclose(objects_lid[0].box3d):\n",
    "    print('We rotated! Original object unchanged.')\n",
    "\n",
    "# -- can be inverted\n",
    "new_box3 = new_box1.rotate(rot1.q.conjugate(), inplace=False)  # inverted with transpose\n",
    "if new_box3.allclose(objects_lid[0].box3d):\n",
    "    print('Back to the original with inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cd193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We translated! Original object unchanged.\n",
      "Translation class and vector are the same!\n",
      "Back to the original with inversion\n"
     ]
    }
   ],
   "source": [
    "# -- translation with a vector\n",
    "t1 = np.array([1, 2, -3])\n",
    "new_box1 = objects_lid[0].box3d.translate(t1, inplace=False)\n",
    "if not new_box1.allclose(objects_lid[0].box3d):\n",
    "    print('We translated! Original object unchanged.')\n",
    "    \n",
    "# -- translation with a Translation class\n",
    "t2 = avstack.geometry.Vector(t1, reference=objects_lid[0].box3d.reference)\n",
    "new_box2 = objects_lid[0].box3d.translate(t2, inplace=False)\n",
    "if new_box1.allclose(new_box2):\n",
    "    print('Translation class and vector are the same!')\n",
    "    \n",
    "# -- can be inverted\n",
    "new_box3 = new_box2.translate(-t2, inplace=False)  # inverted with negative\n",
    "if new_box3.allclose(objects_lid[0].box3d):\n",
    "    print('Back to the original with inversion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10825f",
   "metadata": {},
   "source": [
    "## Ego-Relative Coordinates\n",
    "\n",
    "Some datasets may have coordinates of the ego vehicle. This can be useful for relating the positioning of the ego to other objects in the scene, e.g., for vehicle-to-vehicle or vehicle-to-infrastructure communications. If we have ego coordinates, we can transform objects from global frame to local frame and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d564e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: in carla, sometimes the first few frames \n",
    "# # don't have object (NPC) data while system is initialized\n",
    "# frame = CDM.frames[10]\n",
    "# objs_local = CDM.get_objects(frame, sensor='main_camera')  # ego-relative, camera frame\n",
    "# ego_pos = CDM.get_ego(frame)\n",
    "# objs_global = [obj.local_to_global(ego_pos) for obj in objs_local]  # global-relative, camera frame\n",
    "\n",
    "# print(ego_pos)\n",
    "# print(objs_local[0].position)\n",
    "# print(objs_global[0].position)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
