{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320ea3e9",
   "metadata": {},
   "source": [
    "# Transforming Data\n",
    "\n",
    "One useful element of AVstack is the marriage between data and calibration. You can only imagine the frustration-savings! Let's walk through a few examples where this is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728764c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import avstack\n",
    "import avapi\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_base = '../../lib-avstack-api/data/'\n",
    "obj_data_dir_k = os.path.join(data_base, 'KITTI/object')\n",
    "raw_data_dir_k = os.path.join(data_base, 'KITTI/raw')\n",
    "obj_data_dir_c = os.path.join(data_base, 'CARLA/carla-object-v1')\n",
    "\n",
    "KSM = avapi.kitti.KittiScenesManager(obj_data_dir_k, raw_data_dir_k, convert_raw=False)\n",
    "KDM = KSM.get_scene_dataset_by_index(scene_idx=0)\n",
    "\n",
    "CSM = avapi.carla.CarlaScenesManager(obj_data_dir_c)\n",
    "CDM = CSM.get_scene_dataset_by_index(0)\n",
    "\n",
    "DM = KDM  # let's use KITTI for this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e13c4",
   "metadata": {},
   "source": [
    "## Representing Objects in Different Sensor Frames\n",
    "\n",
    "All objects in AVstack are specified relative to a coordinate frame origin. That means we can easily acquire object representations in different coordinate frames - the work is performed under-the-hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1713d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box3D=[h: 2.17, w: 1.71, l: 4.33] x (x: -8.35 y: 2.09, z: 24.98)\n",
      "  q: quaternion(-0.48376251, 0.49092364, 0.51104843, -0.51361341) with origin: origin of x:[0.    0.06  0.905], q:quaternion(0.49951805, 0.49674862, -0.50538759, 0.49830303) \n",
      "\n",
      "Box3D=[h: 2.17, w: 1.71, l: 4.33] x (x: 25.21 y: 8.60, z: -1.79)\n",
      "  q: quaternion(-0.02126047, -0.00970825, 0.00433318, 0.99971815) with origin: origin of x:[-0.27   0.     0.985], q:quaternion(-0.99991581, 0.00461001, 0.00957989, -0.00743908)\n"
     ]
    }
   ],
   "source": [
    "frame = 0\n",
    "objects_cam = DM.get_objects(frame=frame, sensor='main_camera')\n",
    "objects_lid = DM.get_objects(frame=frame, sensor='main_lidar')\n",
    "print(objects_cam[0].box3d, '\\n')\n",
    "print(objects_lid[0].box3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d3877",
   "metadata": {},
   "source": [
    "We can verify these are nearly equivalent with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0079d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not exactly the same due to rounding errors!\n",
      "But really close to the same!\n"
     ]
    }
   ],
   "source": [
    "obj_L1_cam = objects_lid[0].deepcopy()\n",
    "obj_L1_cam.change_origin(objects_cam[0].origin)\n",
    "\n",
    "# check exact equality\n",
    "if obj_L1_cam.box3d != objects_cam[0].box3d:\n",
    "    print('Not exactly the same due to rounding errors!')\n",
    "\n",
    "# check approximate equality\n",
    "if obj_L1_cam.box3d.allclose(objects_cam[0].box3d):\n",
    "    print('But really close to the same!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939666d3",
   "metadata": {},
   "source": [
    "## Implicit Conversions\n",
    "\n",
    "Some functions in AVstack are smart enough to know that they should convert under the hood when performing certain operations. One example is checking for bounding box overlap - overlap should be independent of the coordinate frame. A similar example is visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3cfdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion handled under the hood!\n",
      "But again, not exact due to rounding errors\n"
     ]
    }
   ],
   "source": [
    "# did we need to do conversion at all??\n",
    "if objects_lid[0].box3d.allclose(objects_cam[0].box3d):\n",
    "    print('Conversion handled under the hood!')\n",
    "\n",
    "# but again, appoximate\n",
    "if objects_lid[0].box3d != objects_cam[0].box3d:\n",
    "    print('But again, not exact due to rounding errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cae399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999792427197"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking bounding box overlap -- also implicitly converted!\n",
    "objects_lid[0].box3d.IoU(objects_cam[0].box3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcb520",
   "metadata": {},
   "source": [
    "## Applying External Transformations\n",
    "\n",
    "We can apply external transformations (rotations and translations) on any object. Rotations are quaternions and translations are 3-vectors. When applied to objects, in both cases, a new object is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae9758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We rotated! Original object unchanged.\n",
      "Rotation class and quaternion are the same!\n",
      "Back to the original with inversion\n"
     ]
    }
   ],
   "source": [
    "# -- rotation with a free-standing quaternion\n",
    "rot1 = avstack.geometry.transformations.transform_orientation([0, 20*np.pi/180, 0], 'euler', 'quat')\n",
    "new_box1 = objects_lid[0].box3d.rotate(rot1)\n",
    "if not new_box1.allclose(objects_lid[0].box3d):\n",
    "    print('We rotated! Original object unchanged.')\n",
    "    \n",
    "# -- rotation with a Rotation class\n",
    "rot2 = avstack.geometry.Rotation(rot1)  # implicit origin of 0's\n",
    "new_box2 = objects_lid[0].box3d.rotate(rot2)\n",
    "if new_box1.allclose(new_box2):\n",
    "    print('Rotation class and quaternion are the same!')\n",
    "    \n",
    "# -- can be inverted\n",
    "new_box3 = new_box2.rotate(rot2.T)  # inverted with transpose\n",
    "if new_box3.allclose(objects_lid[0].box3d):\n",
    "    print('Back to the original with inversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cd193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We translated! Original object unchanged.\n",
      "Translation class and vector are the same!\n",
      "Back to the original with inversion\n"
     ]
    }
   ],
   "source": [
    "# -- translation with a vector\n",
    "t1 = np.array([1, 2, -3])\n",
    "new_box1 = objects_lid[0].box3d.translate(t1)\n",
    "if not new_box1.allclose(objects_lid[0].box3d):\n",
    "    print('We translated! Original object unchanged.')\n",
    "    \n",
    "# -- translation with a Translation class\n",
    "t2 = avstack.geometry.Translation(t1)  # implicit origin of 0's\n",
    "new_box2 = objects_lid[0].box3d.translate(t2)\n",
    "if new_box1.allclose(new_box2):\n",
    "    print('Translation class and vector are the same!')\n",
    "    \n",
    "# -- can be inverted\n",
    "new_box3 = new_box2.translate(-t2)  # inverted with negative\n",
    "if new_box3.allclose(objects_lid[0].box3d):\n",
    "    print('Back to the original with inversion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10825f",
   "metadata": {},
   "source": [
    "## Ego-Relative Coordinates\n",
    "\n",
    "Some datasets may have coordinates of the ego vehicle. This can be useful for relating the positioning of the ego to other objects in the scene, e.g., for vehicle-to-vehicle or vehicle-to-infrastructure communications. If we have ego coordinates, we can transform objects from global frame to local frame and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d564e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VehicleState car at position Translation at [-7.280088, 1.567832, -43.427118] with origin of x:[1.6 0.  1.6], q:quaternion(-0.5, -0.5, 0.5, -0.5)\n",
      "Translation at [0.038995, 1.725757, 21.55844] with origin of x:[1.6 0.  1.6], q:quaternion(-0.5, -0.5, 0.5, -0.5)\n",
      "Translation at [-1.54934607, 45.14448159, 14.23287626] with origin of x:[1.6 0.  1.6], q:quaternion(-0.5, -0.5, 0.5, -0.5)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: in carla, sometimes the first few frames \n",
    "# don't have object (NPC) data while system is initialized\n",
    "frame = CDM.frames[10]\n",
    "objs_local = CDM.get_objects(frame, sensor='main_camera')  # ego-relative, camera frame\n",
    "ego_pos = CDM.get_ego(frame)\n",
    "objs_global = [obj.local_to_global(ego_pos) for obj in objs_local]  # global-relative, camera frame\n",
    "\n",
    "print(ego_pos)\n",
    "print(objs_local[0].position)\n",
    "print(objs_global[0].position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc03684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
