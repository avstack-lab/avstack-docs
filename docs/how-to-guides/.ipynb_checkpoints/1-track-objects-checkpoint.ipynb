{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253c80b9",
   "metadata": {},
   "source": [
    "# Track Objects\n",
    "\n",
    "Let's build on the last perception example and work to integrate longitudinal scenes with a tracker. AVtack provides a few custom tracking implementations with inspiration taken from some literature examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8d0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot import rss library\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import avstack\n",
    "import avapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_base = '../../lib-avstack-api/data/'\n",
    "obj_data_dir_k = os.path.join(data_base, 'KITTI/object')\n",
    "raw_data_dir_k = os.path.join(data_base, 'KITTI/raw')\n",
    "obj_data_dir_c = os.path.join(data_base, 'CARLA/carla-object-v1')\n",
    "\n",
    "KSM = avapi.kitti.KittiScenesManager(obj_data_dir_k, raw_data_dir_k, convert_raw=False)\n",
    "KDM = KSM.get_scene_dataset_by_index(scene_idx=0)\n",
    "\n",
    "CSM = avapi.carla.CarlaScenesManager(obj_data_dir_c)\n",
    "CDM = CSM.get_scene_dataset_by_index(0)\n",
    "\n",
    "DM = KDM  # let's use kitti or this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e11958",
   "metadata": {},
   "source": [
    "## LiDAR-Based Perception and Tracking\n",
    "\n",
    "Ensure that you have the perception model weights downloaded. Following the installation instructions from the last tutorial should do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05afc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/.cache/pypoetry/virtualenvs/avstack-docs-l0eE3ZqO-py3.8/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding default threshold of 0.5 with 0.7\n",
      "load checkpoint from local path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/checkpoints/kitti/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:84: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result_folder = 'results_lidar'\n",
    "P = avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "        model='pointpillars', dataset='kitti', threshold=0.7, \n",
    "        save_output=True, save_folder=result_folder)\n",
    "T = avstack.modules.tracking.tracker3d.BasicBoxTracker(\n",
    "        framerate=DM.framerate, save_output=True, save_folder=result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038e8ce",
   "metadata": {},
   "source": [
    "### Inference over Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496dde50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/108 [00:00<?, ?it/s]/home/spencer/.cache/pypoetry/virtualenvs/avstack-docs-l0eE3ZqO-py3.8/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████████████████████████████████████████████| 108/108 [00:09<00:00, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# detection and tracking\n",
    "for frame in tqdm(DM.frames):\n",
    "    pc = DM.get_lidar(frame, sensor='main_lidar')\n",
    "    \n",
    "    # -- perception\n",
    "    dets = P(pc, frame=frame, identifier='lidar-detections')\n",
    "    \n",
    "    # -- tracking\n",
    "    tracks = T(dets, frame=frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d119a32",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluation can be accelerated using multiprocessing - this is enabled by default in the following commands. Evaluation over a sequence outputs both per-frame metrics and also aggregates these metrics over the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af94f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^^ suppress output\n",
    "percep_res_frames, percep_res_seq, _ = avapi.evaluation.get_percep_results_from_folder(\n",
    "    DM, P.save_folder, sensor_eval='main_lidar')\n",
    "track_res_frames, track_res_seq, track_res_exp = avapi.evaluation.get_track_results_from_folder(\n",
    "    DM, T.save_folder, sensor_eval='main_lidar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fc900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate perception results:\n",
      " {'tot_TP': 340, 'tot_FP': 78, 'tot_FN': 110, 'tot_T': 450, 'mean_precision': 0.8159391534391536, 'mean_recall': 0.7542989417989419} \n",
      "\n",
      "Aggregate tracking results:\n",
      " {'tot_TT': 323, 'tot_FT': 95, 'tot_MT': 127, 'tot_T': 450, 'mean_precision': 0.7642636684303352, 'mean_recall': 0.7223544973544974} \n",
      "\n",
      "Expanded track metrics include:\n",
      " dict_keys(['HOTA_HOTA', 'HOTA_DetA', 'HOTA_AssA', 'HOTA_DetRe', 'HOTA_DetPr', 'HOTA_AssRe', 'HOTA_AssPr', 'HOTA_LocA', 'HOTA_OWTA', 'HOTA_HOTA_TP', 'HOTA_HOTA_FN', 'HOTA_HOTA_FP', 'HOTA_HOTA(0)', 'HOTA_LocA(0)', 'HOTA_HOTALocA(0)', 'CLEAR_MOTA', 'CLEAR_MOTP', 'CLEAR_MODA', 'CLEAR_CLR_Re', 'CLEAR_CLR_Pr', 'CLEAR_MTR', 'CLEAR_PTR', 'CLEAR_MLR', 'CLEAR_sMOTA', 'CLEAR_CLR_F1', 'CLEAR_FP_per_frame', 'CLEAR_MOTAL', 'CLEAR_MOTP_sum', 'CLEAR_CLR_TP', 'CLEAR_CLR_FN', 'CLEAR_CLR_FP', 'CLEAR_IDSW', 'CLEAR_MT', 'CLEAR_PT', 'CLEAR_ML', 'CLEAR_Frag', 'CLEAR_CLR_Frames', 'VACE_STDA', 'VACE_VACE_IDs', 'VACE_VACE_GT_IDs', 'VACE_FDA', 'VACE_num_non_empty_timesteps', 'VACE_ATA', 'VACE_SFDA', 'IDEucl_IDEucl', 'Count_Dets', 'Count_GT_Dets', 'Count_IDs', 'Count_GT_IDs', 'Count_Frames']) \n",
      "\n",
      "     e.g., HOTA_LocA(0): 3.8073170572030963\n"
     ]
    }
   ],
   "source": [
    "print('Aggregate perception results:\\n', percep_res_seq, '\\n')\n",
    "print('Aggregate tracking results:\\n', track_res_seq, '\\n')\n",
    "print('Expanded track metrics include:\\n', track_res_exp.keys(), '\\n')\n",
    "print(f'     e.g., HOTA_LocA(0): {track_res_exp[\"HOTA_LocA(0)\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ef2b9",
   "metadata": {},
   "source": [
    "## Camera-LiDAR Fusion at the Tracking Level\n",
    "\n",
    "Notice how the precision metric is greatly improved using camera-lidar fusion as opposed to lidar-based tracking alone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b2c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding default threshold of 0.5 with 0.7\n",
      "load checkpoint from local path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/checkpoints/kitti/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n",
      "Overriding default threshold of 0.5 with 0.7\n",
      "load checkpoint from local path: /home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection/checkpoints/cityscapes/faster_rcnn_r50_fpn_1x_cityscapes_20200502-829424c0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:84: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result_folder = 'results_fusion'\n",
    "\n",
    "PL = avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "        model='pointpillars', dataset='kitti', threshold=0.7, \n",
    "        save_output=True, save_folder=result_folder)\n",
    "\n",
    "PC = avstack.modules.perception.object2dfv.MMDetObjectDetector2D(\n",
    "        model='fasterrcnn', dataset='kitti', threshold=0.7, \n",
    "        save_output=True, save_folder=result_folder)\n",
    "\n",
    "T_fuse = avstack.modules.tracking.tracker3d.BasicBoxTrackerFusion3Stage(\n",
    "        framerate=DM.framerate, save_output=True, save_folder=result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c50321",
   "metadata": {},
   "source": [
    "### Inference over Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48246aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/108 [00:00<?, ?it/s]/home/spencer/Documents/Projects/AVstack/avstack-docs/lib-avstack-core/third_party/mmdetection/mmdet/datasets/utils.py:66: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████| 108/108 [00:19<00:00,  5.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# detection and tracking\n",
    "for frame in tqdm(DM.frames):\n",
    "    pc = DM.get_lidar(frame, sensor='main_lidar')\n",
    "    img = DM.get_image(frame, sensor='main_camera')\n",
    "    \n",
    "    # -- perception\n",
    "    dets_lid = PL(pc, frame=frame, identifier='lidar-detections')\n",
    "    dets_cam = PC(img, frame=frame, identifier='camera-detections')\n",
    "    \n",
    "    # -- tracking\n",
    "    tracks = T_fuse(detections_3d=dets_lid, detections_2d=dets_cam, frame=frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e9ddd",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd5a506f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ^^ suppress output\n",
    "percep_res_frames, percep_res_seq, _ = avapi.evaluation.get_percep_results_from_folder(\n",
    "    DM, PL.save_folder, sensor_eval='main_lidar')\n",
    "track_res_frames, track_res_seq, track_res_exp = avapi.evaluation.get_track_results_from_folder(\n",
    "    DM, T_fuse.save_folder, sensor_eval='main_lidar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e16412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate perception results:\n",
      " {'tot_TP': 340, 'tot_FP': 78, 'tot_FN': 110, 'tot_T': 450, 'mean_precision': 0.8159391534391536, 'mean_recall': 0.7542989417989419} \n",
      "\n",
      "Aggregate tracking results:\n",
      " {'tot_TT': 236, 'tot_FT': 17, 'tot_MT': 214, 'tot_T': 450, 'mean_precision': 0.6421296296296296, 'mean_recall': 0.4782186948853616} \n",
      "\n",
      "Expanded track metrics include:\n",
      " dict_keys(['HOTA_HOTA', 'HOTA_DetA', 'HOTA_AssA', 'HOTA_DetRe', 'HOTA_DetPr', 'HOTA_AssRe', 'HOTA_AssPr', 'HOTA_LocA', 'HOTA_OWTA', 'HOTA_HOTA_TP', 'HOTA_HOTA_FN', 'HOTA_HOTA_FP', 'HOTA_HOTA(0)', 'HOTA_LocA(0)', 'HOTA_HOTALocA(0)', 'CLEAR_MOTA', 'CLEAR_MOTP', 'CLEAR_MODA', 'CLEAR_CLR_Re', 'CLEAR_CLR_Pr', 'CLEAR_MTR', 'CLEAR_PTR', 'CLEAR_MLR', 'CLEAR_sMOTA', 'CLEAR_CLR_F1', 'CLEAR_FP_per_frame', 'CLEAR_MOTAL', 'CLEAR_MOTP_sum', 'CLEAR_CLR_TP', 'CLEAR_CLR_FN', 'CLEAR_CLR_FP', 'CLEAR_IDSW', 'CLEAR_MT', 'CLEAR_PT', 'CLEAR_ML', 'CLEAR_Frag', 'CLEAR_CLR_Frames', 'VACE_STDA', 'VACE_VACE_IDs', 'VACE_VACE_GT_IDs', 'VACE_FDA', 'VACE_num_non_empty_timesteps', 'VACE_ATA', 'VACE_SFDA', 'IDEucl_IDEucl', 'Count_Dets', 'Count_GT_Dets', 'Count_IDs', 'Count_GT_IDs', 'Count_Frames']) \n",
      "\n",
      "     e.g., HOTA_LocA(0): 3.83519657442073\n"
     ]
    }
   ],
   "source": [
    "print('Aggregate perception results:\\n', percep_res_seq, '\\n')\n",
    "print('Aggregate tracking results:\\n', track_res_seq, '\\n')\n",
    "print('Expanded track metrics include:\\n', track_res_exp.keys(), '\\n')\n",
    "print(f'     e.g., HOTA_LocA(0): {track_res_exp[\"HOTA_LocA(0)\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
